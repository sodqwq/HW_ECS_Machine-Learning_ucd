{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ceeeb46",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "\n",
    "For this assignment, you will be developing an artificial neural network to classify data given in the __[Dry Beans Data Set](https://archive.ics.uci.edu/ml/datasets/Dry+Bean+Dataset#)__. This data set was obtained as a part of a research study by Selcuk University, Turkey, in which a computer vision system was developed to distinguish seven different registered varieties of dry beans with similar features. More details on the study can be found in the following __[research paper](https://www.sciencedirect.com/science/article/pii/S0168169919311573)__.\n",
    "\n",
    "## About the Data Set For Q1\n",
    "Seven different types of dry beans were used in a study in Selcuk University, Turkey, taking into account the features such as form, shape, type, and structure by the market situation. A computer vision system was developed to distinguish seven different registered varieties of dry beans with similar features in order to obtain uniform seed classification. For the classification model, images of 13611 grains of 7 different registered dry beans were taken with a high-resolution camera. Bean images obtained by computer vision system were subjected to segmentation and feature extraction stages, and a total of 16 features - 12 dimensions and 4 shape forms - were obtained from the grains.\n",
    "\n",
    "Number of Instances (records in the data set): __13611__\n",
    "\n",
    "Number of Attributes (fields within each record, including the class): __17__\n",
    "\n",
    "### Data Set Attribute Information:\n",
    "\n",
    "1. __Area (A)__ : The area of a bean zone and the number of pixels within its boundaries.\n",
    "2. __Perimeter (P)__ : Bean circumference is defined as the length of its border.\n",
    "3. __Major axis length (L)__ : The distance between the ends of the longest line that can be drawn from a bean.\n",
    "4. __Minor axis length (l)__ : The longest line that can be drawn from the bean while standing perpendicular to the main axis.\n",
    "5. __Aspect ratio (K)__ : Defines the relationship between L and l.\n",
    "6. __Eccentricity (Ec)__ : Eccentricity of the ellipse having the same moments as the region.\n",
    "7. __Convex area (C)__ : Number of pixels in the smallest convex polygon that can contain the area of a bean seed.\n",
    "8. __Equivalent diameter (Ed)__ : The diameter of a circle having the same area as a bean seed area.\n",
    "9. __Extent (Ex)__ : The ratio of the pixels in the bounding box to the bean area.\n",
    "10. __Solidity (S)__ : Also known as convexity. The ratio of the pixels in the convex shell to those found in beans.\n",
    "11. __Roundness (R)__ : Calculated with the following formula: (4piA)/(P^2)\n",
    "12. __Compactness (CO)__ : Measures the roundness of an object: Ed/L\n",
    "13. __ShapeFactor1 (SF1)__\n",
    "14. __ShapeFactor2 (SF2)__\n",
    "15. __ShapeFactor3 (SF3)__\n",
    "16. __ShapeFactor4 (SF4)__\n",
    "\n",
    "17. __Classes : *Seker, Barbunya, Bombay, Cali, Dermosan, Horoz, Sira*__\n",
    "\n",
    "### Libraries that can be used :\n",
    "- NumPy, SciPy, Pandas, Sci-Kit Learn, TensorFlow, Keras\n",
    "- Any other library used during the lectures and discussion sessions.\n",
    "\n",
    "## About the Data Set For Q3\n",
    "In this problem, we will be exploring the car dataset and analyzing their fuel efficiency. <br >\n",
    "Specifically, we will do some exploratory analysis with visualizations, then we will build a model for Simple Linear Regression, a model for Polynomial Regression, and one model for Logistic Regression. <br >\n",
    "**The given dataset is already modified and cleaned**, but you can find [the original information here.](https://archive.ics.uci.edu/ml/datasets/auto+mpg).\n",
    "\n",
    "## Dataset Attribute Information\n",
    "\n",
    "1. **mpg**: Miles per gallon. This is one primary measurement for car fuel efficiency.\n",
    "2. **displacement** : The cylinder volumes in cubic inches.\n",
    "3. **horsepower** : Engine power.\n",
    "4. **weight** : In pounds.\n",
    "5. **acceleration** : The elapsed time in seconds to go from 0 to 60mph.\n",
    "6. **origin** : Region of origin.\n",
    "\n",
    "### Libraries that can be used: numpy, pandas, scikit-learn, seaborn, plotly, matplotlib\n",
    "Any libraries used in the discussion materials are also allowed.\n",
    "\n",
    "\n",
    "### Other Notes\n",
    "- Don't worry about not being able to achieve high accuracy, it is neither the goal nor the grading standard of this assignment.\n",
    "- Discussion materials should be helpful for doing the assignments.\n",
    "- The homework submission should be a .ipynb file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847a75c7",
   "metadata": {},
   "source": [
    "## Exercise 1 : Data Understanding (23 points in total)\n",
    "* As the classes are categorical, use one-hot encoding to represent the set of classes. \n",
    "* Normalize each field of the input data using the min-max normalization technique.\n",
    "* Plot the distribution of data and analyze the distribution. Explain if the data is symmetric, or sekewed to right or left. \n",
    "* Plot the target (class) variable and show if data has linear or non-linear behavior. Suggest whether a linear or non-linear model should be developed for the data. \n",
    "    * hints: After normalizing the input variables, for each pair of input variables (xi, xj), create a 2D pair-plot between the variables, and color-code the target variable according to the label of the target variable (for example you can use red for y=\"sira\"). Then analyze the relationship between the input variables and the target variable to see if you can find a linear no non-linear line to separate the data points according to the target variable class. For each combination of input variables, create a separate pair plot. Pair plot has been discussed in the HW1 discussion. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7491c1",
   "metadata": {},
   "source": [
    "## Exercise 2 - Logistic Regression (20 points in total)\n",
    "Recall the dataset from last week homework\n",
    "\n",
    "Now we are going to build a classification model on ``origin`` using all the other 5 attributes. <br >\n",
    "Note that Logistic Regression is a binary classificaiton algorithm.\n",
    "\n",
    "### Exercise 2.1 - Processing and Splitting the Dataset (5 points)\n",
    "In this exercise 3, we only consider those observations where they originate from either \"USA\" or \"Japan\". <br >\n",
    "So please **remove** those observations that originate from \"Europe\". <br >\n",
    "And then, split the data into training and testing set with the ratio of 80:20. <br >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89374dbf",
   "metadata": {},
   "source": [
    "### Exercise 2.2 - Logistic Regression (15 points)\n",
    "\n",
    "Using all the other 5 attributes, please build a Logistic Regression model that distinguishes between cars from Japan and cars from the USA. <br >\n",
    "Then, **if we are distinguishing between Japan and Europe this time, how do you think the model performance(in terms of accuracy) will change? Provide your reasoning.** (Hint: Exercise 1)\n",
    "\n",
    "Requirements\n",
    " - Report the testing precision and recall for both regions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91f7416",
   "metadata": {},
   "source": [
    "## Exercise 3 - Polynomial Regressor using Gradient Descent (20 points in total)\n",
    "Now we are going to look into model fitting. In the dataset cost.csv, the first column is the independent variable production_output, and the second column is the dependent variable cost.\n",
    "\n",
    "### Exercise 3.1 - Split the dataset (5 points)\n",
    "Import the dataset cost.csv and split them into training and testing set with ratio 70:30."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531d7f98",
   "metadata": {},
   "source": [
    "### Exercise 3.2 - Polynomial Regression (15 points)\n",
    "Compute the RMSE and R2 for the training and testing set. Using polynomial regression with degree 1, 2, 3, and 4, which model provides the most appropriate prediction? Justify your answer and plot the models fitted line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b248b8",
   "metadata": {},
   "source": [
    "## Exercise 4 : Log Likelihood (27 points in total)\n",
    "For a model with two independent variables x1 (Weight) and x2 (Length), compute the log-likelihood for the model: ŷ = 0.1 x1 + 0.1 x2 , where the dependent variable ŷ represents the prediction for Height. Given the dataset below (in the table), assuming that the measurements have normal distribution, please complete the table. \n",
    "\n",
    "Round your answer to 4 d.p. For simplicity, use and log base 10.\n",
    "\n",
    "Hint: $f(y_i)=\\frac{1}{\\sqrt{2\\piσ^2}}e^{\\frac{-(y_i-ŷ_i)^2}{2σ^2}}$\n",
    "\n",
    "Model 1: \n",
    "Weight x1 | Length x2 | Actual Height y | Predicted Height ŷ | squared residual (y - ŷ )^2 | Log-Likelihood\n",
    "---|---|---|---|---|---|\n",
    "7.0 | 50 | 5.80\n",
    "6.0 | 55 | 5.70\n",
    "8.0 | 56 | 6.00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111edc02",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
